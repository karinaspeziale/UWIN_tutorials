} else {
return(
list(
classification = NA,
retired = FALSE
)
)
}
}
# Retire once 5 users tag empty
five_empty <- function(sp_tag){
unq_sp <- unique(sp_tag)
finished_loc <- rep(NA, length(unq_sp))
for(i in 1:length(unq_sp)){
where_sp <- cumsum(
sp_tag == unq_sp[i]
)
if(7 %in% where_sp & unq_sp[i] != "Empty"){
finished_loc[i] <- which(where_sp == 7)[1]
}
if(5 %in% where_sp & unq_sp[i] == "Empty"){
finished_loc[i] <- which(where_sp == 5)[1]
}
}
if("Empty" %in% unq_sp){
where_empty <- which(unq_sp == "Empty")
if(!is.na(finished_loc[where_empty])){
if(min(finished_loc, na.rm = TRUE) == finished_loc[where_empty]){
return(
list(
classification = "Empty",
retired = TRUE
)
)
} else {
return(
list(
classification = NA,
retired = FALSE
)
)
}
} else {
return(
list(
classification = NA,
retired = FALSE
)
)
}
} else {
return(
list(
classification = NA,
retired = FALSE
)
)
}
}
# Retire once 7 users tag the same species
any_sp_7 <- function(sp_tag){
unq_sp <- unique(sp_tag)
tag_finished <- sapply(
1:length(unq_sp),
function(x){
where_sp <- cumsum(
sp_tag == unq_sp[x]
)
if(7 %in% where_sp){
return(which(where_sp == 7)[1])
} else {
return(NA)
}
}
)
if(any(!is.na(tag_finished))){
tag_finished[is.na(tag_finished)] <- 999
to_return <- which(
tag_finished == min(tag_finished)
)
return(
list(
classification = as.character(
unq_sp
)[to_return],
retired = TRUE
)
)
}else{
return(
list(
classification = NA,
retired = FALSE
)
)
}
}
# Retire once 15 users tag a photo
class_count <- function(sp_tag, expert){
my_count <- table(sp_tag)
if(sum(my_count) >=15){
my_count <- sort(
my_count,
decreasing = TRUE
)
if(sum(my_count == max(my_count)) == 1){
sp <- names(my_count)[1]
}else{
sp <- expert
}
return(
list(
classification = sp,
retired = TRUE
)
)
} else {
return(
list(
classification = NA,
retired = FALSE
)
)
}
}
# Retire if any user tags human
human_tag <- function(sp_tag){
if("Human" %in% sp_tag){
return(
list(
classification = "Human",
retired = TRUE
)
)
} else {
return(
list(
classification = NA,
retired = FALSE
)
)
}
}
# Put these functions into a loop
pb <- txtProgressBar(max = length(zoo_list))
for(i in 52251:length(zoo_list)){
setTxtProgressBar(pb, i)
# DO THE SORTING BY ID DATE
zoo_list[[i]] = zoo_list[[i]] %>%
dplyr::arrange(retired.created_at)
if(nrow(zoo_list[[i]]) < 3){
if(!"Human" %in% zoo_list[[i]]$choice){
zoo_list[[i]]$our_rt <- "Error"
zoo_list[[i]]$classification <- "Error"
next
}
}
# human check
is_human <- human_tag(zoo_list[[i]]$choice)
if(is_human$retired){
zoo_list[[i]]$our_rt <- "Human"
zoo_list[[i]]$final_tag <- paste0(
is_human$classification,
collapse = ", "
)
next
}
# empty check
is_empty <- first_3_empty(zoo_list[[i]]$choice)
if(is_empty$retired){
zoo_list[[i]]$our_rt <- "3_nothing_here"
zoo_list[[i]]$final_tag <- is_empty$classification
next
}
is_5_empty <- five_empty(zoo_list[[i]]$choice)
if(is_5_empty$retired){
zoo_list[[i]]$our_rt <- "5_nothing_here"
zoo_list[[i]]$final_tag <- is_5_empty$classification
next
}
# species check
sp_7_tags <- any_sp_7(zoo_list[[i]]$choice)
if(sp_7_tags$retired){
zoo_list[[i]]$our_rt <- "7_species"
zoo_list[[i]]$final_tag <- paste0(
sp_7_tags$classification,
collapse = ", "
)
next
}
# classification count check
class_count_15 <- class_count(zoo_list[[i]]$choice, as.character("expert review")) #second part needs updating
if(class_count_15$retired){
zoo_list[[i]]$our_rt <- "classification_count"
zoo_list[[i]]$final_tag <- paste0(
class_count_15$classification,
collapse = ", "
)
next
}
}
# Unlist your data
retire_23 <- dplyr::bind_rows(zoo_list)
# Unlist your data
retire_23 <- dplyr::bind_rows(zoo_list)
# drop NA userIDs, which have been changed to -1
if("-1" %in% retire_23$user_id){
retire_23 <- retire_23[-which(retire_23$user_id == "-1"),]
if("Error" %in% retire_23$our_rt){
retire_23 <- retire_23[-which(retire_23$our_rt == "Error"),]
}
}
retire_23 <- retire_23 %>%
filter(!user_id == -1)
retire_23 <- retire_23 %>%
filter(!is.na(our_rt))
saveRDS(retire_23, file = "CHIL23_cleaned")
head(retire_23)
colnames(retire_23)
# match column names
retire_23 <- retire_23 %>%
rename(photoName = image_1)
CHIL_sites <- data.table::fread("CHIL_sites.csv", data.table = FALSE)
colnames(CHIL_sites)
View(CHIL_sites)
CHIL_sites <- data.table::fread("CHIL_sites.csv", data.table = FALSE, header = TRUE)
# join data
retire_23 <- left_join(retire_23, CHIL_sites, by = "photoName")
View(retire_23)
# Now sort species information for each site
site_sum = retire_23 %>%
group_by(locationAbbr, landOwnerName) %>%
summarise(final_tag)
View(site_sum)
# Now sort species information for each site
site_sum = retire_23 %>%
group_by(locationAbbr, landOwnerName) %>%
summarise(n=(final_tag))
View(site_sum)
# Now sort species information for each site
site_sum = retire_23 %>%
group_by(locationAbbr, landOwnerName) %>%
summarise(count(final_tag))
# Now sort species information for each site
site_sum = retire_23 %>%
group_by(locationAbbr, landOwnerName) %>%
summarise(n=())
# Now sort species information for each site
site_sum = retire_23 %>%
group_by(locationAbbr, landOwnerName) %>%
summarise(species = n())
# Now sort species information for each site
site_sum = retire_23 %>%
group_by(locationAbbr, landOwnerName) %>%
summarise(species = n(unique(final_tag)))
# Now sort species information for each site
site_sum = retire_23 %>%
group_by(locationAbbr, landOwnerName) %>%
summarise(species = count(unique(final_tag)))
# Now sort species information for each site
site_sum = retire_23 %>%
group_by(locationAbbr, landOwnerName, final_tag) %>%
summarise(species = count(unique(final_tag)))
# Now sort species information for each site
site_sum = retire_23 %>%
group_by(locationAbbr, landOwnerName) %>%
summarise(species = length(unique(final_tag)))
# Now sort species information for each site
site_sum = retire_23 %>%
group_by(locationAbbr, landOwnerName) %>%
summarise(species_det = length(unique(final_tag)))
site_sp = retire_23 %>%
group_by(locationAbbr) %>%
summarise(species =(unique(final_tag)))
View(site_sp)
summary_table_2023 <- left_join(site_sp, site_sum, by = "locationAbbr")
View(summary_table_2023)
# save for later
write.csv(summary_table_2023, "summary_table_2023.csv")
site_list = split(summary_table_2023, factor(summary_table_2023$locationAbbr))
length(unique(summary_table_2023$locationAbbr))
lapply(names(site_list), function(df) write.csv(lst[[df]], file=paste0("site_data/"), df, ".csv") )
site_list[[1]]
getwd()
for (df in names(site_list)) write.csv(lst[[df]], file=paste0("site_data/"), df, ".csv")
lapply(names(site_list), function(df) write.csv(lst[[df]], file=paste0("site_data/"), df, ".csv"))
lapply(names(site_list), function(df) write.csv(site_list[[df]], file=paste0("site_data/"), df, ".csv"))
site_list$`D02-BMT1`
(1:length(site_list))
df = (1:length(site_list))
lapply(names(site_list), function(df) write.csv(site_list[[df]], file=paste0("site_data/"), df, ".csv"))
df = (1:length(site_list))
for (df in names(site_list)) write.csv(site_list[[df]], file=paste0("site_data/"), df, ".csv")
for (df in length(site_list)) write.csv(site_list[[df]], file=paste0("site_data/"), df, ".csv")
for (df in names(site_list)) write.csv(site_list[[df]], file=paste0("site_data/"), df, ".csv")
site_list[[1]]
names(site_list)
lapply(names(site_list), function(df) write.csv(site_list[[df]], file=paste0("site_data/"), df, ".csv"))
site_list[[1]]
# output .csv for each location
lapply(names(site_list), function(df) write.csv(site_list[[df]], file=paste0("site_data/"), df, ".csv"))
getwd()
# output .csv for each location
lapply(names(site_list), function(df) write.csv(site_list[[df]], file=paste0("/site_data/"), df, ".csv"))
names(site_list)
site_list[["D02-BMT1"]]
# output .csv for each location
lapply(names(site_list), function(df) write.csv(site_list[[df]], df, ".csv"))
for (df in names(site_list)) write.csv(site_list[[df]], file=paste0("site_data/"), df, ".csv")
# output .csv for each location
lapply(names(site_list), function(df), write.csv(site_list[[df]], file=paste0("site_data/"), df, ".csv"))
getwd()
# output .csv for each location
lapply(names(site_list), function(df) write.csv(site_list[[df]], file=paste0("E:/LPZ Coordinator/Zooniverse/2023_data/site_data/"), df, ".csv"))
# output .csv for each location
lapply(names(site_list), function(df) write.csv(site_list[[df]], file=paste0("E:/LPZ Coordinator/Zooniverse/2023_data/"), df, ".csv"))
# output .csv for each location
lapply(names(site_list), function(df) write.csv(site_list[[df]], file=paste0("E:/LPZ Coordinator/Zooniverse/2023_data"), df, ".csv"))
# Load in libraries
library(dplyr)
library(ggplot2)
# Helpful references
# https://doi90.github.io/lodestar/fitting-occupancy-models-with-unmarked.html
# file:///E:/LPZ%20Coordinator/uwin_R/Lesson_6_Occupancy%20modeling.pdf
# Tutorial guide
# https://github.com/ourcodingclub/tutorials-in-progress/blob/master/Tutorial_publishing_guide.md
# Set your local working directory
setwd("E:/GitHub/UWIN_tutorials/tutorials/static occupancy")
raccoon <- read.csv("chicago_raccoon.csv", head = TRUE, skip = 3) # we use skip to deal with first 3 lines of
head(raccoon)                                                     # notes on start and end date
# We see that this data contains information from 170 sites. We can choose to consider each 'day'
# as a 'visit' or, if our species are rare or hard to detect, we can collapse each visit into
# multiple days. Given the large 'zero' or 'unoccupied' occurrence of raccoons, we will collapse each visit into a ~6 day visits
# let's confirm that there are no repeated sites
length(unique(raccoon$Site))
# Great. Now let's collapse our data into 6-day sampling visits. We can do this a couple of ways...
# 1. We can manually collapse days into weekly visits by summing selected rows...
# First we need to make sure we remove all rows with only NA's, otherwise our next summing function
# will convert those NA's to zeros
# # Filter out rows with all NA's
# raccoon_wk <- filter(raccoon, rowSums(is.na(raccoon[7:37])) != ncol(raccoon[7:37]))
#
# # Below is problematic b/c changes NAs to zeros
# raccoon_wk <- raccoon_wk %>%
#   mutate(visit_1 = select(., Day_1:Day_6) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_2 = select(., Day_7:Day_12) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_3 = select(., Day_13:Day_18) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_4 = select(., Day_19:Day_24) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_5 = select(., Day_25:Day_31) %>% rowSums(na.rm = TRUE)) %>%
#   select(-c(Day_1:Day_31))
#
# # Then changing counts >0 to '1' and count = 0, to '0'
# raccoon_wk <- raccoon_wk %>%
#   mutate(visit_1 = ifelse(visit_1 >= 1, 1, 0)) %>%
#   mutate(visit_2 = ifelse(visit_2 >= 1, 1, 0)) %>%
#   mutate(visit_3 = ifelse(visit_3 >= 1, 1, 0)) %>%
#   mutate(visit_4 = ifelse(visit_4 >= 1, 1, 0)) %>%
#   mutate(visit_5 = ifelse(visit_5 >= 1, 1, 0))
# OR, we could use a loop to do this all at once...Mason?
# get all columns that start with day
day_cols <- raccoon[,grep("^Day_",colnames(raccoon))]
# split them into six day groups
n_weeks <- ceiling(ncol(day_cols)/6)
week_groups <- rep(1:n_weeks, each = 6)[1:ncol(day_cols)]
combine_days <- function(y, groups){
ans <- rep(NA, max(groups))
for(i in 1:length(groups)){
tmp <- as.numeric(y[groups == i])
if(all(is.na(tmp))){
next
} else {
ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)
}
}
return(ans)
}
week_summary <- t( # this transposes our matrix
apply(
day_cols,
1, # 1 is for rows
combine_days,
groups = week_groups
)
)
colnames(week_summary) <- paste0("Week_",1:n_weeks)
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk <- cbind(raccoon_wk, week_summary)
# Now one issue that may arrive from these groupings is that when occasion lengths don't evenly
# break down into our total sampling days, we may have uneven occasions lengths as done above. Here we have
# five, 6 day occasions, and one, 1 day occasion. We can either combine this last day into the fifth occasion
# or drop that day. For now, let's drop that last sampling day
raccoon_wk <- raccoon_wk %>%
select(-Week_6)
# Though raccoons have found ways to adapt to urban ecosystems, we hypothesize that
# raccoon occupancy will be highest with proximity to forests and water sources given
# their preference for wooded and wetlands areas to den and forage. To model this, let's use
# the National Land Cover Database developed by the United States Geological Survey and join it with our data
# https://www.usgs.gov/centers/eros/science/national-land-cover-database
# This dataset was extracted using the FedData package. Column values are the % landcover types
# within 1000m of each site coordinate
# chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cran.r-project.org/web/packages/FedData/FedData.pdf
landcover <- read.csv("Chicago_NLCD_landcover.csv", head = TRUE)
head(landcover)
# we need to make sure 'sites' are named the same to join these datasets
colnames(raccoon_wk)
colnames(landcover)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Now we can join our datasets and drop columns where there are NA site covarites
raccoon_wk <- left_join(raccoon_wk, landcover, by = 'Site') %>%
na.omit(.)
library("unmarked")
?unmarkedFrameOccu()
y <- raccoon_wk %>%
select(Week_1:Week_5)
siteCovs <- raccoon_wk %>%
select(c(water, forest))
# We should also examine our covariates of interest to see how they distribute
png("siteCovs_water.png", height = 700, width = 700)
hist(siteCovs$water)
dev.off()
png("siteCovs_forest.png", height = 700, width = 700)
hist(siteCovs$forest)
dev.off()
hist(siteCovs$forest)
### Make a comment that 170 sites comes from environment NOT head() function
### also check out skim package
install.packages("skimr")
library(skimr)
skim(raccoon)
# Let's confirm that there are no repeated sites
length(unique(raccoon$Site))
# Great, no repeats! Now let's collapse our data into 6-day sampling occasions. Let's grab all the columns that start with day...
day_cols <- raccoon[,grep("^Day_",colnames(raccoon))]
# split them into six day groups...
n_weeks <- ceiling(ncol(day_cols)/6)
week_groups <- rep(1:n_weeks, each = 6)[1:ncol(day_cols)]
# and write a function that keeps each occasion with all NA's as such and those > 0 as 1
### and write a function that keeps each occasion with all NA's as such and those with all 0's as 0,
# and those with at least 1 detection, as 1
combine_days <- function(y, groups){
ans <- rep(NA, max(groups))
for(i in 1:length(groups)){
tmp <- as.numeric(y[groups == i])
if(all(is.na(tmp))){
next
} else {
ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)
}
}
return(ans)
}
# Apply this function across rows (in groups of 6)
week_summary <- t( # this transposes our matrix
apply(
day_cols,
1, # 1 is for rows
combine_days,
groups = week_groups
)
)
# Now update names
colnames(week_summary) <- paste0("Week_",1:n_weeks)
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk <- cbind(raccoon_wk, week_summary)
raccoon_wk <- raccoon_wk %>%
select(-Week_6)
landcover <- read.csv("Chicago_NLCD_landcover.csv", head = TRUE)
head(landcover)
# Let's join this dataset to our raccoon data. First we need to make sure 'sites' are named the same to join these datasets
colnames(raccoon_wk)
colnames(landcover)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Now we can join our datasets and drop NA's.
raccoon_wk <- left_join(raccoon_wk, landcover, by = 'Site') %>%
na.omit(.)
install.packages("unmarked")
library("unmarked")
?unmarked()
?unmarkedFrameOccu()
y <- raccoon_wk %>%
select(Week_1:Week_5)
siteCovs <- raccoon_wk %>%
select(c(water, forest))
# We should also examine our covariates of interest to see if they should be scaled
hist(raccoon_wk$water)
ggsave("plots/water_hist.png", width = 4)
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
hist(raccoon_wk$forest)
ggsave("plots/forest_hist.png", width = 4)
ggplot(raccoon_wk, aes(x = forest)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion forest", y = "Site count")
install.packages("unmarked")
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
ggsave("plots/water_hist.png", width = 4)
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
