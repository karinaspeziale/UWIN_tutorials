mutate(Income = scale(Income))
# now we can model our predictions across gradients of Impervious cover and Income
opo_imperv <- predict(
object = m2,
type = "psi",
newdata = imperv_scale
)
opo_income <- predict(
object = m2,
type = "psi",
newdata = income_scale
)
# We can also use ggplot to pretty up the plot
# first merge the two datasets (predicted occupancy and impervious cover)
imperv_plot <- bind_cols(opo_imperv, imperv) %>%
select(-c(Income))
ggplot(imperv_plot, aes(x = Impervious, y = estimate)) +
geom_ribbon(aes(ymin = lower, ymax = upper), fill = "orange", alpha = 0.5) +
geom_path(size = 1) + # adds line
labs(x = "Impervious cover", y = "Occupancy probability") +
ggtitle("Opossum Occupancy")+
scale_x_continuous(limits = c(20,80)) +
ylim(0,1)+
theme_classic()+ # drops gray background and grid
theme(plot.title=element_text(hjust=0.5)) # centers titles
# we can get more creative with our colors using the package `colourpicker`
library(colourpicker)
# go to your 'Addins' tab and select `colourpicker`.
ggsave("plots/opo_imperv_ggplot.jpg", width = 6, height = 6)
ggplot(imperv_plot, aes(x = Impervious, y = estimate)) +
geom_ribbon(aes(ymin = lower, ymax = upper), fill = "#72AD8F", alpha = 0.5) +
geom_path(size = 1) + # adds line
labs(x = "Impervious cover", y = "Occupancy probability") +
ggtitle("Opossum Occupancy with Impervious Cover")+
scale_x_continuous(limits = c(20,80)) +
ylim(0,1)+
theme_classic()+ # drops gray background and grid
theme(plot.title=element_text(size = 16, hjust=0.5), # centers titles
axis.text.x = element_text(size = 12),
axis.text.y = element_text(size = 12),
axis.title = element_text(size = 18))
income_plot <- bind_cols(opo_income, income) %>%
select(-c(Impervious))
ggplot(income_plot, aes(x = Income, y = estimate)) +
geom_ribbon(aes(ymin = lower, ymax = upper), fill = c("#D4B282"), alpha = 0.5) +
geom_path(size = 1) + # adds line
labs(x = "Per Captia Income (US Dollar)", y = "Occupancy probability") +
ggtitle("Opossum Occupancy with Income")+
scale_x_continuous(limits = c(28000,80000)) +
ylim(0,1)+
theme_classic()+ # drops gray background and grid
theme(plot.title=element_text(size = 16, hjust=0.5), # centers titles
axis.text.x = element_text(size = 12),
axis.text.y = element_text(size = 12),
axis.title = element_text(size = 18))
# Seasonal model----------------------------------------------------------------
# Make a data.frame based on actual data
unique(season_frame$Season)
season <- m3@estimates[1:4,]
season[,1] <- c("January", "April", "July", "October")
ggplot(data = season, aes(x = parameter, y = Est)) +
xlab("\nMonth in 2019") +
ylab("Opossum ccupancy\n")+
scale_y_continuous(limits = c(-2,2)) +
geom_point(data = season, aes(x=parameter, y = Est))+
geom_errorbar(aes(ymin=lower, ymax=upper), size = 1, width=.2, color= "#094C59") +
geom_point(color= "#094C59", size = 6, shape = 19)
# Load in libraries
library(dplyr)
library(ggplot2)
# Helpful references
# https://doi90.github.io/lodestar/fitting-occupancy-models-with-unmarked.html
# file:///E:/LPZ%20Coordinator/uwin_R/Lesson_6_Occupancy%20modeling.pdf
# Tutorial guide
# https://github.com/ourcodingclub/tutorials-in-progress/blob/master/Tutorial_publishing_guide.md
# Set your local working directory
setwd("E:/GitHub/UWIN_tutorials/tutorials/static occupancy")
raccoon <- read.csv("chicago_raccoon.csv", head = TRUE, skip = 3) # we use skip to deal with first 3 lines of
head(raccoon)                                                     # notes on start and end date
# We see that this data contains information from 170 sites. We can choose to consider each 'day'
# as a 'visit' or, if our species are rare or hard to detect, we can collapse each visit into
# multiple days. Given the large 'zero' or 'unoccupied' occurrence of raccoons, we will collapse each visit into a ~6 day visits
# let's confirm that there are no repeated sites
length(unique(raccoon$Site))
# Great. Now let's collapse our data into 6-day sampling visits. We can do this a couple of ways...
# 1. We can manually collapse days into weekly visits by summing selected rows...
# First we need to make sure we remove all rows with only NA's, otherwise our next summing function
# will convert those NA's to zeros
# # Filter out rows with all NA's
# raccoon_wk <- filter(raccoon, rowSums(is.na(raccoon[7:37])) != ncol(raccoon[7:37]))
#
# # Below is problematic b/c changes NAs to zeros
# raccoon_wk <- raccoon_wk %>%
#   mutate(visit_1 = select(., Day_1:Day_6) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_2 = select(., Day_7:Day_12) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_3 = select(., Day_13:Day_18) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_4 = select(., Day_19:Day_24) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_5 = select(., Day_25:Day_31) %>% rowSums(na.rm = TRUE)) %>%
#   select(-c(Day_1:Day_31))
#
# # Then changing counts >0 to '1' and count = 0, to '0'
# raccoon_wk <- raccoon_wk %>%
#   mutate(visit_1 = ifelse(visit_1 >= 1, 1, 0)) %>%
#   mutate(visit_2 = ifelse(visit_2 >= 1, 1, 0)) %>%
#   mutate(visit_3 = ifelse(visit_3 >= 1, 1, 0)) %>%
#   mutate(visit_4 = ifelse(visit_4 >= 1, 1, 0)) %>%
#   mutate(visit_5 = ifelse(visit_5 >= 1, 1, 0))
# OR, we could use a loop to do this all at once...Mason?
# get all columns that start with day
day_cols <- raccoon[,grep("^Day_",colnames(raccoon))]
# split them into six day groups
n_weeks <- ceiling(ncol(day_cols)/6)
week_groups <- rep(1:n_weeks, each = 6)[1:ncol(day_cols)]
combine_days <- function(y, groups){
ans <- rep(NA, max(groups))
for(i in 1:length(groups)){
tmp <- as.numeric(y[groups == i])
if(all(is.na(tmp))){
next
} else {
ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)
}
}
return(ans)
}
week_summary <- t( # this transposes our matrix
apply(
day_cols,
1, # 1 is for rows
combine_days,
groups = week_groups
)
)
colnames(week_summary) <- paste0("Week_",1:n_weeks)
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk <- cbind(raccoon_wk, week_summary)
# Now one issue that may arrive from these groupings is that when occasion lengths don't evenly
# break down into our total sampling days, we may have uneven occasions lengths as done above. Here we have
# five, 6 day occasions, and one, 1 day occasion. We can either combine this last day into the fifth occasion
# or drop that day. For now, let's drop that last sampling day
raccoon_wk <- raccoon_wk %>%
select(-Week_6)
# Though raccoons have found ways to adapt to urban ecosystems, we hypothesize that
# raccoon occupancy will be highest with proximity to forests and water sources given
# their preference for wooded and wetlands areas to den and forage. To model this, let's use
# the National Land Cover Database developed by the United States Geological Survey and join it with our data
# https://www.usgs.gov/centers/eros/science/national-land-cover-database
# This dataset was extracted using the FedData package. Column values are the % landcover types
# within 1000m of each site coordinate
# chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cran.r-project.org/web/packages/FedData/FedData.pdf
landcover <- read.csv("Chicago_NLCD_landcover.csv", head = TRUE)
head(landcover)
# we need to make sure 'sites' are named the same to join these datasets
colnames(raccoon_wk)
colnames(landcover)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Now we can join our datasets and drop columns where there are NA site covarites
raccoon_wk <- left_join(raccoon_wk, landcover, by = 'Site') %>%
na.omit(.)
library("unmarked")
?unmarkedFrameOccu()
y <- raccoon_wk %>%
select(visit_1:visit_5)
siteCovs <- raccoon_wk %>%
select(c(water, forest))
# We should also examine our covariates of interest to see how they distribute
png("siteCovs_water.png", height = 700, width = 700)
hist(siteCovs$water)
dev.off()
png("siteCovs_forest.png", height = 700, width = 700)
hist(siteCovs$forest)
dev.off()
# We probably want to scale these covariates
siteCovs <- siteCovs %>%
mutate(water_scale = scale(water)) %>%
mutate(forest_scale = scale(forest)) %>%
select(-c(water, forest))
# Make sure this is a dataframe object
siteCovs_df <- data.frame(siteCovs)
raccoon_occ <- unmarkedFrameOccu(y = y, siteCovs = siteCovs_df)
summary(raccoon_occ)
# Be mindful that it is OK to have missing or NA observation data. BUT for each observation
# there must be affiliated covariate data, otherwise this data will not be considered in the model.
# We only have landcover data for 119/170 sites so we will see these sites dropped in our model
# Fitting models
# Let's fit two models, one for a null hypothesis:
# null: raccoon occupancy is constant across sites
# habitat hypothesis: raccoon occupancy is explained habitat metrics, water and forest,
# where occupancy increased with increasing proportions of water and forests
?occu()
null_model <- occu(~1 # detection
~1, # occupancy
data = raccoon_occ)
null_model
habitat_model <- occu(~1 # detection
~ forest_scale + water_scale, # occupancy
data = raccoon_occ)
habitat_model
# Now we want to compare our models. Thankfully `unmarked` has a function for that
fitlist <- fitList(m1 = null_model, m2 = habitat_model)
modSel(fitlist)
# it looks like our null model best explains our data. Let's look at our model parameters
# for detection and occupancy probabilities
plogis(coef(null_model, type = "state")) # for occupancy
plogis(coef(null_model, type = "det")) # for detection
# Do above but also include 95% confidence intervals
occ_error <- cbind(coef(null_model, type = "state"),
confint(null_model, type = "state"))
# do same for detection
det_error <- cbind(coef(null_model, type = "det"),
confint(null_model, type = "det"))
# convert back to probability
plogis(occ_error)
plogis(det_error)
# Set your local working directory
setwd("E:/GitHub/UWIN_tutorials/tutorials/static occupancy")
raccoon <- read.csv("chicago_raccoon.csv", head = TRUE, skip = 3) # we use skip to deal with first 3 lines of
head(raccoon)                                                     # notes on start and end date
# let's confirm that there are no repeated sites
length(unique(raccoon$Site))
# OR, we could use a loop to do this all at once...Mason?
# get all columns that start with day
day_cols <- raccoon[,grep("^Day_",colnames(raccoon))]
# split them into six day groups
n_weeks <- ceiling(ncol(day_cols)/6)
week_groups <- rep(1:n_weeks, each = 6)[1:ncol(day_cols)]
combine_days <- function(y, groups){
ans <- rep(NA, max(groups))
for(i in 1:length(groups)){
tmp <- as.numeric(y[groups == i])
if(all(is.na(tmp))){
next
} else {
ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)
}
}
return(ans)
}
week_summary <- t( # this transposes our matrix
apply(
day_cols,
1, # 1 is for rows
combine_days,
groups = week_groups
)
)
colnames(week_summary) <- paste0("Week_",1:n_weeks)
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk <- cbind(raccoon_wk, week_summary)
raccoon_wk <- raccoon_wk %>%
select(-Week_6)
# This dataset was extracted using the FedData package. Column values are the % landcover types
# within 1000m of each site coordinate
# chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cran.r-project.org/web/packages/FedData/FedData.pdf
landcover <- read.csv("Chicago_NLCD_landcover.csv", head = TRUE)
head(landcover)
# we need to make sure 'sites' are named the same to join these datasets
colnames(raccoon_wk)
colnames(landcover)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Now we can join our datasets and drop columns where there are NA site covarites
raccoon_wk <- left_join(raccoon_wk, landcover, by = 'Site') %>%
na.omit(.)
library("unmarked")
y <- raccoon_wk %>%
select(visit_1:visit_5)
y <- raccoon_wk %>%
dplyr::select(visit_1:visit_5)
View(raccoon)
View(raccoon_wk)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Set your local working directory
setwd("E:/GitHub/UWIN_tutorials/tutorials/static occupancy")
raccoon <- read.csv("chicago_raccoon.csv", head = TRUE, skip = 3) # we use skip to deal with first 3 lines of
head(raccoon)                                                     # notes on start and end date
# We see that this data contains information from 170 sites. We can choose to consider each 'day'
# as a 'visit' or, if our species are rare or hard to detect, we can collapse each visit into
# multiple days. Given the large 'zero' or 'unoccupied' occurrence of raccoons, we will collapse each visit into a ~6 day visits
# let's confirm that there are no repeated sites
length(unique(raccoon$Site))
# Great. Now let's collapse our data into 6-day sampling visits. We can do this a couple of ways...
# 1. We can manually collapse days into weekly visits by summing selected rows...
# First we need to make sure we remove all rows with only NA's, otherwise our next summing function
# will convert those NA's to zeros
# # Filter out rows with all NA's
# raccoon_wk <- filter(raccoon, rowSums(is.na(raccoon[7:37])) != ncol(raccoon[7:37]))
#
# # Below is problematic b/c changes NAs to zeros
# raccoon_wk <- raccoon_wk %>%
#   mutate(visit_1 = select(., Day_1:Day_6) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_2 = select(., Day_7:Day_12) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_3 = select(., Day_13:Day_18) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_4 = select(., Day_19:Day_24) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_5 = select(., Day_25:Day_31) %>% rowSums(na.rm = TRUE)) %>%
#   select(-c(Day_1:Day_31))
#
# # Then changing counts >0 to '1' and count = 0, to '0'
# raccoon_wk <- raccoon_wk %>%
#   mutate(visit_1 = ifelse(visit_1 >= 1, 1, 0)) %>%
#   mutate(visit_2 = ifelse(visit_2 >= 1, 1, 0)) %>%
#   mutate(visit_3 = ifelse(visit_3 >= 1, 1, 0)) %>%
#   mutate(visit_4 = ifelse(visit_4 >= 1, 1, 0)) %>%
#   mutate(visit_5 = ifelse(visit_5 >= 1, 1, 0))
# OR, we could use a loop to do this all at once...Mason?
# get all columns that start with day
day_cols <- raccoon[,grep("^Day_",colnames(raccoon))]
# split them into six day groups
n_weeks <- ceiling(ncol(day_cols)/6)
week_groups <- rep(1:n_weeks, each = 6)[1:ncol(day_cols)]
combine_days <- function(y, groups){
ans <- rep(NA, max(groups))
for(i in 1:length(groups)){
tmp <- as.numeric(y[groups == i])
if(all(is.na(tmp))){
next
} else {
ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)
}
}
return(ans)
}
week_summary <- t( # this transposes our matrix
apply(
day_cols,
1, # 1 is for rows
combine_days,
groups = week_groups
)
)
colnames(week_summary) <- paste0("Week_",1:n_weeks)
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk <- cbind(raccoon_wk, week_summary)
# Now one issue that may arrive from these groupings is that when occasion lengths don't evenly
# break down into our total sampling days, we may have uneven occasions lengths as done above. Here we have
# five, 6 day occasions, and one, 1 day occasion. We can either combine this last day into the fifth occasion
# or drop that day. For now, let's drop that last sampling day
raccoon_wk <- raccoon_wk %>%
select(-Week_6)
# Though raccoons have found ways to adapt to urban ecosystems, we hypothesize that
# raccoon occupancy will be highest with proximity to forests and water sources given
# their preference for wooded and wetlands areas to den and forage. To model this, let's use
# the National Land Cover Database developed by the United States Geological Survey and join it with our data
# https://www.usgs.gov/centers/eros/science/national-land-cover-database
# This dataset was extracted using the FedData package. Column values are the % landcover types
# within 1000m of each site coordinate
# chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cran.r-project.org/web/packages/FedData/FedData.pdf
landcover <- read.csv("Chicago_NLCD_landcover.csv", head = TRUE)
head(landcover)
# we need to make sure 'sites' are named the same to join these datasets
colnames(raccoon_wk)
colnames(landcover)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Now we can join our datasets and drop columns where there are NA site covarites
raccoon_wk <- left_join(raccoon_wk, landcover, by = 'Site') %>%
na.omit(.)
raccoon_wk
y <- raccoon_wk %>%
select(Week_1:Week_5)
siteCovs <- raccoon_wk %>%
select(c(water, forest))
hist(siteCovs$water)
# Make sure this is a data.frame object
siteCovs_df <- data.frame(siteCovs)
raccoon_occ <- unmarkedFrameOccu(y = y, siteCovs = siteCovs_df)
null_model <- occu(~1 # detection
~1, # occupancy
data = raccoon_occ)
habitat_model <- occu(~1 # detection
~ forest_scale + water_scale, # occupancy
data = raccoon_occ)
# Now we want to compare our models. Thankfully `unmarked` has a function for that
fitlist <- fitList(m1 = null_model, m2 = habitat_model)
View(raccoon_occ)
# We probably want to scale these covariates
siteCovs <- siteCovs %>%
mutate(water_scale = scale(water)) %>%
mutate(forest_scale = scale(forest)) %>%
select(-c(water, forest))
# Make sure this is a data.frame object
siteCovs_df <- data.frame(siteCovs)
raccoon_occ <- unmarkedFrameOccu(y = y, siteCovs = siteCovs_df)
summary(raccoon_occ)
setwd("E:/GitHub/UWIN_tutorials/tutorials/static occupancy")
raccoon <- read.csv("chicago_raccoon.csv", head = TRUE, skip = 3) # we use skip to deal with first 3 lines of
head(raccoon)                                                     # notes on start and end date
# We see that this data contains information from 170 sites. We can choose to consider each 'day'
# as a 'visit' or, if our species are rare or hard to detect, we can collapse each visit into
# multiple days. Given the large 'zero' or 'unoccupied' occurrence of raccoons, we will collapse each visit into a ~6 day visits
# let's confirm that there are no repeated sites
length(unique(raccoon$Site))
# Great. Now let's collapse our data into 6-day sampling visits. We can do this a couple of ways...
# 1. We can manually collapse days into weekly visits by summing selected rows...
# First we need to make sure we remove all rows with only NA's, otherwise our next summing function
# will convert those NA's to zeros
# # Filter out rows with all NA's
# raccoon_wk <- filter(raccoon, rowSums(is.na(raccoon[7:37])) != ncol(raccoon[7:37]))
#
# # Below is problematic b/c changes NAs to zeros
# raccoon_wk <- raccoon_wk %>%
#   mutate(visit_1 = select(., Day_1:Day_6) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_2 = select(., Day_7:Day_12) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_3 = select(., Day_13:Day_18) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_4 = select(., Day_19:Day_24) %>% rowSums(na.rm = TRUE)) %>%
#   mutate(visit_5 = select(., Day_25:Day_31) %>% rowSums(na.rm = TRUE)) %>%
#   select(-c(Day_1:Day_31))
#
# # Then changing counts >0 to '1' and count = 0, to '0'
# raccoon_wk <- raccoon_wk %>%
#   mutate(visit_1 = ifelse(visit_1 >= 1, 1, 0)) %>%
#   mutate(visit_2 = ifelse(visit_2 >= 1, 1, 0)) %>%
#   mutate(visit_3 = ifelse(visit_3 >= 1, 1, 0)) %>%
#   mutate(visit_4 = ifelse(visit_4 >= 1, 1, 0)) %>%
#   mutate(visit_5 = ifelse(visit_5 >= 1, 1, 0))
# OR, we could use a loop to do this all at once...Mason?
# get all columns that start with day
day_cols <- raccoon[,grep("^Day_",colnames(raccoon))]
# split them into six day groups
n_weeks <- ceiling(ncol(day_cols)/6)
week_groups <- rep(1:n_weeks, each = 6)[1:ncol(day_cols)]
combine_days <- function(y, groups){
ans <- rep(NA, max(groups))
for(i in 1:length(groups)){
tmp <- as.numeric(y[groups == i])
if(all(is.na(tmp))){
next
} else {
ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)
}
}
return(ans)
}
week_summary <- t( # this transposes our matrix
apply(
day_cols,
1, # 1 is for rows
combine_days,
groups = week_groups
)
)
colnames(week_summary) <- paste0("Week_",1:n_weeks)
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk <- cbind(raccoon_wk, week_summary)
# Now one issue that may arrive from these groupings is that when occasion lengths don't evenly
# break down into our total sampling days, we may have uneven occasions lengths as done above. Here we have
# five, 6 day occasions, and one, 1 day occasion. We can either combine this last day into the fifth occasion
# or drop that day. For now, let's drop that last sampling day
raccoon_wk <- raccoon_wk %>%
select(-Week_6)
# Though raccoons have found ways to adapt to urban ecosystems, we hypothesize that
# raccoon occupancy will be highest with proximity to forests and water sources given
# their preference for wooded and wetlands areas to den and forage. To model this, let's use
# the National Land Cover Database developed by the United States Geological Survey and join it with our data
# https://www.usgs.gov/centers/eros/science/national-land-cover-database
# This dataset was extracted using the FedData package. Column values are the % landcover types
# within 1000m of each site coordinate
# chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cran.r-project.org/web/packages/FedData/FedData.pdf
landcover <- read.csv("Chicago_NLCD_landcover.csv", head = TRUE)
head(landcover)
# we need to make sure 'sites' are named the same to join these datasets
colnames(raccoon_wk)
colnames(landcover)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Now we can join our datasets and drop columns where there are NA site covarites
raccoon_wk <- left_join(raccoon_wk, landcover, by = 'Site') %>%
na.omit(.)
library("unmarked")
?unmarkedFrameOccu()
y <- raccoon_wk %>%
select(Week_1:Week_5)
siteCovs <- raccoon_wk %>%
select(c(water, forest))
# We probably want to scale these covariates
siteCovs <- siteCovs %>%
mutate(water_scale = scale(water)) %>%
mutate(forest_scale = scale(forest)) %>%
select(-c(water, forest))
View(siteCovs)
View(siteCovs_df)
# Make sure this is a data.frame object
siteCovs_df <- data.frame(siteCovs)
raccoon_occ <- unmarkedFrameOccu(y = y, siteCovs = siteCovs_df)
null_model <- occu(~1 # detection
~1, # occupancy
data = raccoon_occ)
habitat_model <- occu(~1 # detection
~ forest_scale + water_scale, # occupancy
data = raccoon_occ)
# Now we want to compare our models. Thankfully `unmarked` has a function for that
fitlist <- fitList(m1 = null_model, m2 = habitat_model)
modSel(fitlist)
# it looks like our null model best explains our data. Let's look at our model parameters
# for detection and occupancy probabilities
plogis(coef(null_model, type = "state")) # for occupancy
plogis(coef(null_model, type = "det")) # for detection
# Do above but also include 95% confidence intervals
occ_error <- cbind(coef(null_model, type = "state"),
confint(null_model, type = "state"))
# do same for detection
det_error <- cbind(coef(null_model, type = "det"),
confint(null_model, type = "det"))
# convert back to probability
plogis(occ_error)
plogis(det_error)
siteValue <- apply(X = y,
MARGIN = 1, # 1 = across rows
FUN = "max", na.rm = TRUE) # This function finds the max value
mean(siteValue)
