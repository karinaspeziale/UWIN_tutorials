INNER JOIN Photos ph ON ph.visitID = vi.visitID
INNER JOIN Detections de ON de.photoName = ph.photoName
WHERE NOT sa.areaAbbr IN('CHMF', 'FAKE')"
cities <- SELECT(qry)
cities <- cities[order(cities$areaAbbr),]
cities <- cities[order(cities$areaAbbr),]
city_info <- vector(
"list",
length = nrow(cities)
)
pb <- txtProgressBar(max = nrow(cities))
for(city in 1:ncity){
setTxtProgressBar(pb, i)
response <- try(
SELECT(
paste0(
"SELECT DISTINCT s.commonName, v.locationID, v.visitID, c.locationAbbr, p.photoDatetime, d.valStatID FROM Photos p \n",
"INNER JOIN Visits v ON p.visitID=v.visitID AND p.photoDateTime >= v.activeStart AND p.photoDateTime <= v.activeEnd\n",
"INNER JOIN CameraLocations c ON (v.locationID=c.locationID)\n",
"INNER JOIN Detections d ON (p.photoName=d.photoName)",
"INNER JOIN DetectionSpecies ds ON (d.detectionID=ds.detectionID)\n",
"INNER JOIN Species s ON (ds.speciesID=s.speciesID)\n",
"WHERE c.areaID = ", cities$areaID[city],"\n",
"\n AND d.valStatID IN (1,2)"
)
),
silent = TRUE
)
if(is.data.frame(response)){
city_info[[city]] <- response
city_info[[city]]$city <- cities$areaAbbr[city]
}
}
ncity <- nrow(cities)
pb <- txtProgressBar(max = nrow(cities))
for(city in 1:ncity){
setTxtProgressBar(pb, i)
response <- try(
SELECT(
paste0(
"SELECT DISTINCT s.commonName, v.locationID, v.visitID, c.locationAbbr, p.photoDatetime, d.valStatID FROM Photos p \n",
"INNER JOIN Visits v ON p.visitID=v.visitID AND p.photoDateTime >= v.activeStart AND p.photoDateTime <= v.activeEnd\n",
"INNER JOIN CameraLocations c ON (v.locationID=c.locationID)\n",
"INNER JOIN Detections d ON (p.photoName=d.photoName)",
"INNER JOIN DetectionSpecies ds ON (d.detectionID=ds.detectionID)\n",
"INNER JOIN Species s ON (ds.speciesID=s.speciesID)\n",
"WHERE c.areaID = ", cities$areaID[city],"\n",
"\n AND d.valStatID IN (1,2)"
)
),
silent = TRUE
)
if(is.data.frame(response)){
city_info[[city]] <- response
city_info[[city]]$city <- cities$areaAbbr[city]
}
}
pb <- txtProgressBar(max = nrow(cities))
for(city in 1:ncity){
setTxtProgressBar(pb, city)
response <- try(
SELECT(
paste0(
"SELECT DISTINCT s.commonName, v.locationID, v.visitID, c.locationAbbr, p.photoDatetime, d.valStatID FROM Photos p \n",
"INNER JOIN Visits v ON p.visitID=v.visitID AND p.photoDateTime >= v.activeStart AND p.photoDateTime <= v.activeEnd\n",
"INNER JOIN CameraLocations c ON (v.locationID=c.locationID)\n",
"INNER JOIN Detections d ON (p.photoName=d.photoName)",
"INNER JOIN DetectionSpecies ds ON (d.detectionID=ds.detectionID)\n",
"INNER JOIN Species s ON (ds.speciesID=s.speciesID)\n",
"WHERE c.areaID = ", cities$areaID[city],"\n",
"\n AND d.valStatID IN (1,2)"
)
),
silent = TRUE
)
if(is.data.frame(response)){
city_info[[city]] <- response
city_info[[city]]$city <- cities$areaAbbr[city]
}
}
city_info <- dplyr::bind_rows(city_info)
unique(city_info$commonName)
library(tidyverse)
city_info <- city_info %>%
filter(commonName != "Empty")
gc
gc()
city_info <- city_info %>%
filter(commonName != "Empty") %>%
filter(commonName != "Human")
colnames(city_info)
common_sp <- city_info %>%
group_by(city, commonName) %>%
summarise(sp_count = length(commonName))
View(common_sp)
common_sp <- city_info %>%
group_by(city, commonName) %>%
summarise(sp_count = length(commonName)) %>%
ungroup() %>%
group_by(city) %>%
arrange(sp_count)
View(common_sp)
?arrange()
common_sp <- city_info %>%
group_by(city, commonName) %>%
summarise(sp_count = length(commonName)) %>%
ungroup() %>%
group_by(city) %>%
desc(sp_count)
common_sp <- city_info %>%
group_by(city, commonName) %>%
summarise(sp_count = length(commonName)) %>%
ungroup() %>%
group_by(city) %>%
arrange(desc(sp_count))
View(common_sp)
common_sp <- city_info %>%
group_by(city, commonName) %>%
summarise(sp_count = length(commonName)) %>%
ungroup() %>%
group_by(city) %>%
arrange(city, desc(sp_count))
city_info <- city_info %>%
filter(commonName != "Empty") %>%
filter(commonName != "Human") %>%
filter(commonName != "Vehicle") %>%
filter(commonName != "Unknown")
common_sp <- city_info %>%
group_by(city, commonName) %>%
summarise(sp_count = length(commonName)) %>%
ungroup() %>%
group_by(city) %>%
arrange(city, desc(sp_count))
View(common_sp)
city_info <- city_info %>%
filter(commonName != "Empty") %>%
filter(commonName != "Human") %>%
filter(commonName != "Vehicle") %>%
filter(commonName != "Researcher") %>%
filter(commonName != "Unknown")
common_sp <- city_info %>%
group_by(city, commonName) %>%
summarise(sp_count = length(commonName)) %>%
ungroup() %>%
group_by(city) %>%
arrange(city, desc(sp_count))
city_info <- city_info %>%
filter(commonName != "Empty") %>%
filter(commonName != "Human") %>%
filter(commonName != "Vehicle") %>%
filter(commonName != "Researcher") %>%
filter(commonName != "Bird (cannot ID)") %>%
filter(commonName != "Unknown")
common_sp <- city_info %>%
group_by(city, commonName) %>%
summarise(sp_count = length(commonName)) %>%
ungroup() %>%
group_by(city) %>%
arrange(city, desc(sp_count))
city_info <- city_info %>%
filter(commonName != "Empty") %>%
filter(commonName != "Human") %>%
filter(commonName != "Vehicle") %>%
filter(commonName != "Researcher") %>%
filter(commonName != "Bird (cannot ID)") %>%
filter(commonName != "Insect (Cannot ID)") %>%
filter(commonName != "fox (cannot ID)") %>%
filter(commonName != "butterfly (cannot ID)") %>%
filter(commonName != "Rabbit (cannot ID)") %>%
filter(commonName != "Tree squirrel (cannot ID)") %>%
filter(commonName != "small mammal (cannot ID)") %>%
filter(commonName != "skunk (cannot ID)") %>%
filter(commonName != "Deer (cannot ID)") %>%
filter(commonName != "Rabbit (cannot ID)") %>%
filter(commonName != "Lizard (cannot ID)") %>%
filter(commonName != "Snake (cannot ID)") %>%
filter(commonName != "Unknown")
common_sp <- city_info %>%
group_by(city, commonName) %>%
summarise(sp_count = length(commonName)) %>%
ungroup() %>%
group_by(city) %>%
arrange(city, desc(sp_count))
write.csv(common_sp, "E:/GitHub/LPZ Coordinator/Committees/Engagement _ Outreach Committee/common_sp.csv")
common_sp = common_sp %>%
rename(AHGA = Athens)
common_sp = common_sp %>%
rename(Athens = AHGA)
common_sp = common_sp %>%
rename("Athens" = "AHGA")
common_sp = common_sp %>%
rename(Athens = "AHGA")
common_sp = rename(common_ps, Athens = AHGA)
common_sp = rename(common_sp, Athens = AHGA)
class(common_sp)
?rename()
common_sp = rename(common_sp, "Athens" = "AHGA")
common_sp = common_sp %>%
mutate(city = case_when(
city == "AHGA" ~ "Athens"
))
my_cities <- SELECT(
"select sa.areaName, sa.areaAbbr from StudyAreas"
)
my_cities <- SELECT(
"select sa.areaName, sa.areaAbbr from StudyAreas sa"
)
View(my_cities)
my_cities = rename(my_cities, city = areaAbbr)
common_sp = left_join(common_sp, my_cities, by = city)
common_sp = left_join(common_sp, my_cities, by = "city")
View(common_sp)
?left_join()
common_sp = left_join(common_sp, my_cities, by = "city", copy = FALSE)
common_sp <- city_info %>%
group_by(city, commonName) %>%
summarise(sp_count = length(commonName)) %>%
ungroup() %>%
group_by(city) %>%
arrange(city, desc(sp_count))
common_sp = left_join(common_sp, my_cities, by = "city", copy = FALSE)
View(common_sp)
write.csv(common_sp, "E:/GitHub/LPZ Coordinator/Committees/Engagement _ Outreach Committee/common_sp.csv")
write.csv(common_sp, "E:/GitHub/LPZ Coordinator/Committees/Engagement _ Outreach Committee/common_sp.csv")
#Libraries
library(dplyr)
# Reading in file
Emp = read.csv("E:/GitHub/CC-3-DataManip/EmpetrumElongation.csv)
# Load in libraries
library(dplyr)
library(ggplot2)
# Set your local working directory
setwd("E:/GitHub/UWIN_tutorials/tutorials/static occupancy")
raccoon <- read.csv("chicago_raccoon.csv", head = TRUE, skip = 3)
# Check out what data we're working with.
head(raccoon)
skim(raccoon)
### Make a comment that 170 sites comes from environment NOT head() function
### also check out skim package
install.packages("skim")
library(skim)
### Make a comment that 170 sites comes from environment NOT head() function
### also check out skim package
install.packages("skimr")
library(skimr)
skim(raccoon)
# Let's confirm that there are no repeated sites
length(unique(raccoon$Site))
# Great, no repeats! Now let's collapse our data into 6-day sampling occasions. Let's grab all the columns that start with day...
day_cols <- raccoon[,grep("^Day_",colnames(raccoon))]
# split them into six day groups...
n_weeks <- ceiling(ncol(day_cols)/6)
week_groups <- rep(1:n_weeks, each = 6)[1:ncol(day_cols)]
# and write a function that keeps each occasion with all NA's as such and those > 0 as 1
combine_days <- function(y, groups){
ans <- rep(NA, max(groups))
for(i in 1:length(groups)){
tmp <- as.numeric(y[groups == i])
if(all(is.na(tmp))){
next
} else {
ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)
}
}
return(ans)
}
# Apply this function across rows (in groups of 6)
week_summary <- t( # this transposes our matrix
apply(
day_cols,
1, # 1 is for rows
combine_days,
groups = week_groups
)
)
# Now update names
colnames(week_summary) <- paste0("Week_",1:n_weeks)
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk <- cbind(raccoon_wk, week_summary)
raccoon_wk <- raccoon_wk %>%
select(-Week_6)
View(raccoon_wk)
library("unmarked")
?unmarkedFrameOccu()
?unmarked()
install.packages("unmarked")
?unmarked:FrameOccu()
?unmarkedFrameOccu()
?unmarked()
library("unmarked")
?unmarked()
?unmarkedFrameOccu()
y <- raccoon_wk %>%
select(visit_1:visit_5)
# Now update names
colnames(week_summary) <- paste0("Week_",1:n_weeks)
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk <- cbind(raccoon_wk, week_summary)
raccoon_wk <- raccoon_wk %>%
select(-Week_6)
landcover <- read.csv("Chicago_NLCD_landcover.csv", head = TRUE)
head(landcover)
# Let's join this dataset to our raccoon data. First we need to make sure 'sites' are named the same to join these datasets
colnames(raccoon_wk)
colnames(landcover)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Now we can join our datasets and drop NA's.
raccoon_wk <- left_join(raccoon_wk, landcover, by = 'Site') %>%
na.omit(.)
y <- raccoon_wk %>%
select(visit_1:visit_5)
View(raccoon_wk)
y <- raccoon_wk %>%
select(Week_1:Week_5)
# We should also examine our covariates of interest to see if they should be scaled
hist(raccoon_wk$water)
hist(raccoon_wk$forest)
class(raccoon_wk$water)
View(raccoon_wk)
# scale covariates
siteCovs <- siteCovs %>%
mutate(water_scale = scale(water)) %>%
mutate(forest_scale = scale(forest)) %>%
select(-c(water, forest)) # drop unscaled covariates
# Load in libraries
library(dplyr)
# scale covariates
siteCovs <- siteCovs %>%
mutate(water_scale = scale(water)) %>%
mutate(forest_scale = scale(forest)) %>%
select(-c(water, forest)) # drop unscaled covariates
siteCovs <- siteCovs %>%
dplyr::mutate(water_scale = scale(water))
# scale covariates
siteCovs <- siteCovs %>%
dplyr::mutate(water_scale = scale(water)) %>%
dplyr::mutate(forest_scale = scale(forest)) %>%
select(-c(water, forest)) # drop unscaled covariates
# Load in libraries
library(dplyr)
library(ggplot2)
# Set your local working directory
setwd("E:/GitHub/UWIN_tutorials/tutorials/static occupancy")
raccoon <- read.csv("chicago_raccoon.csv", head = TRUE, skip = 3)
# Check out what data we're working with.
head(raccoon)
### Make a comment that 170 sites comes from environment NOT head() function
### also check out skim package
install.packages("skimr")
library(skimr)
skim(raccoon)
# Let's confirm that there are no repeated sites
length(unique(raccoon$Site))
# Great, no repeats! Now let's collapse our data into 6-day sampling occasions. Let's grab all the columns that start with day...
day_cols <- raccoon[,grep("^Day_",colnames(raccoon))]
# split them into six day groups...
n_weeks <- ceiling(ncol(day_cols)/6)
week_groups <- rep(1:n_weeks, each = 6)[1:ncol(day_cols)]
# and write a function that keeps each occasion with all NA's as such and those > 0 as 1
### and write a function that keeps each occasion with all NA's as such and those with all 0's as 0,
# and those with at least 1 detection, as 1
combine_days <- function(y, groups){
ans <- rep(NA, max(groups))
for(i in 1:length(groups)){
tmp <- as.numeric(y[groups == i])
if(all(is.na(tmp))){
next
} else {
ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)
}
}
return(ans)
}
# Apply this function across rows (in groups of 6)
week_summary <- t( # this transposes our matrix
apply(
day_cols,
1, # 1 is for rows
combine_days,
groups = week_groups
)
)
# Now update names
colnames(week_summary) <- paste0("Week_",1:n_weeks)
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk <- cbind(raccoon_wk, week_summary)
raccoon_wk <- raccoon_wk %>%
select(-Week_6)
landcover <- read.csv("Chicago_NLCD_landcover.csv", head = TRUE)
head(landcover)
# Let's join this dataset to our raccoon data. First we need to make sure 'sites' are named the same to join these datasets
colnames(raccoon_wk)
colnames(landcover)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Now we can join our datasets and drop NA's.
raccoon_wk <- left_join(raccoon_wk, landcover, by = 'Site') %>%
na.omit(.)
install.packages("unmarked")
library("unmarked")
?unmarked()
?unmarkedFrameOccu()
y <- raccoon_wk %>%
select(Week_1:Week_5)
siteCovs <- raccoon_wk %>%
select(c(water, forest))
# We should also examine our covariates of interest to see if they should be scaled
hist(raccoon_wk$water)
hist(raccoon_wk$forest)
install.packages("skimr")
install.packages("unmarked")
# scale covariates
siteCovs <- siteCovs %>%
mutate(water_scale = scale(water)) %>%
mutate(forest_scale = scale(forest)) %>%
select(-c(water, forest)) # drop unscaled covariates
siteCovs_df <- data.frame(siteCovs)
# Now we can make our unmarkedFrameOccu() dataframe
raccoon_occ <- unmarkedFrameOccu(y = y, siteCovs = siteCovs_df)
summary(raccoon_occ)
null_model <- occu(~1 # detection
~1, # occupancy
data = raccoon_occ)
habitat_model <- occu(~1 # detection
~ forest_scale + water_scale, # occupancy
data = raccoon_occ)
null_model
habitat_model
# Let's look at
plogis(coef(null_model, type = "state")) # for occupancy
plogis(coef(intercept_model, type = "det")) # for detection
plogis(coef(null_model, type = "det")) # for detection
# Let's look at
plogis(coef(null_model, type = "state")) # for occupancy
plogis(coef(null_model, type = "det")) # for detection
?occu()
fitlist <- fitList(m1 = null_model, m2 = habitat_model)
modSel(fitlist)
fitlist <- fitList(m1 = null_model, m2 = habitat_model)
modSel(fitlist)
plogis(coef(null_model, type = "state")) # for occupancy
plogis(coef(null_model, type = "det")) # for detection
# We can also use `confit` to calculate the associated error
# 95% confidence intervals for occupancy
occ_error <- cbind(coef(null_model, type = "state"),
confint(null_model, type = "state"))
# 95% confidence intervals for detection
det_error <- cbind(coef(null_model, type = "det"),
confint(null_model, type = "det"))
# Convert back to probability
plogis(occ_error)
plogis(det_error)
occ_error
# Our naive occupancy
siteValue <- apply(X = y,
MARGIN = 1, # 1 = across rows
FUN = "max", na.rm = TRUE) # This function finds the max value
mean(siteValue)
# Create a new dataframe
new_dat <- data.frame(forest = seq(from = 0, to = 1, by = 0.05),
water_scale = mean(siteCovs$water_scale))
# Scale the data
new_dat <- new_dat %>%
mutate(forest_scale = scale(forest))
# Make predictions with these data
pred_forest <- predict(habitat_model, type = "state", newdata = new_dat)
head(pred_forest)
plot(pred_forest$Predicted ~ new_dat$forest_scale, # y-axis ~ x-axis
type = "l",  # plot out a line
bty = "l", # box type is an L around plot
xlab = "Scaled proportion forest", # x label
ylab = "Occupancy", # y label
ylim = c(0, 1), # range to y axis
lwd = 2, # width of the line
las = 1 # have numbers on y axis be vertical
)
# add 95% confidence intervals
lines(pred_forest$lower ~ new_dat$forest_scale, # y-axis ~ x-axis
lty = 2 # make a checked line
)
lines(pred_forest$upper ~ new_dat$forest_scale, # y-axis ~ x-axis
lty = 2 # make a checked line
)
# first merge the two datasets (predicted occupancy and forest data)
all_dat <- bind_cols(pred_forest, new_dat)
ggplot(all_dat, aes(x = forest_scale, y = Predicted)) +
geom_ribbon(aes(ymin = lower, ymax = upper), fill = "orange", alpha = 0.5) +
geom_path(size = 1) + # adds line
labs(x = "Proportion forest (scaled)", y = "Occupancy probability") +
ggtitle("Raccoon Occupancy")+
scale_x_continuous(limits = c(0,1)) +
ylim(0,1)+
theme_classic()+ # drops gray background and grid
theme(plot.title=element_text(hjust=0.5)) # centers titles
plot(pred_forest$Predicted ~ new_dat$forest_scale, # y-axis ~ x-axis
type = "l",  # plot out a line
bty = "l", # box type is an L around plot
xlab = "Scaled proportion forest", # x label
ylab = "Occupancy", # y label
ylim = c(0, 1), # range to y axis
xlim = c(0,1),
lwd = 2, # width of the line
las = 1 # have numbers on y axis be vertical
)
# add 95% confidence intervals
lines(pred_forest$lower ~ new_dat$forest_scale, # y-axis ~ x-axis
lty = 2 # make a checked line
)
lines(pred_forest$upper ~ new_dat$forest_scale, # y-axis ~ x-axis
lty = 2 # make a checked line
)
