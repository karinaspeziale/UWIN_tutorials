shinyApp(ui = ui, server = server)
library(ggplot2)  # For creating pretty plots
# Run the app ----
shinyApp(ui = ui, server = server)
library(dplyr)  # For filtering and manipulating data
runApp('E:/GitHub/Coding Club/CC-11-Shiny/Example_App')
# For interactive map
library(tidyverse)
library(sf)
library(mapview)
partner_coord <- read_csv("E:/GitHub/Partner_Tracking/UWIN_city_coords.csv")
partner_coord$Latitude <- as.double(partner_coord$Latitude)
partner_coord$Longitude <- as.double(partner_coord$Longitude)
mapview(partner_coord, xcol = "Longitude", ycol = "Latitude", crs = 4326,
grid = FALSE, layer.name = "UWIN partners", color = "gray", alpha = 0)
# For flat map
library(ggplot2)
library(dplyr)
partner_df <- as.data.frame(partner_coord, region = "City")
coords = sf::st_as_sf(               #sf = spatial tool
partner_coord,
coords = c("Longitude", "Latitude"),
crs = 4326)
sort(unique(ggplot2::map_data("world")$region)) #region options
world_map = map_data("world")
USA = map_data("world", region = "USA")
Canada = map_data("world", region = "Canada")
Germany = map_data("world", region = "Germany")
South.Africa = map_data("world", region = "South Africa")
Madagascar = map_data("world", region = "Madagascar")
Mexico = map_data("world", region = "Mexico")
# colors: http://sape.inf.usi.ch/quick-reference/ggplot2/colour
map <- ggplot(world_map, aes(x = long, y = lat, group = group)) +
geom_polygon(fill="gray", colour = "white")+
geom_polygon(data = USA, fill = "#CCCC66")+
geom_polygon(data = Canada, fill = "#669933")+
geom_polygon(data = Germany, fill = "#FFCC66")+
geom_polygon(data = South.Africa, fill = "#99CC00")+
geom_polygon(data = Madagascar, fill = "#CC9966")+
geom_polygon(data = Mexico, fill = "#CC6633")+
geom_point(data = partner_df, aes(x = Longitude, y = Latitude), inherit.aes = FALSE, pch = 21, fill = "black", color = "black")+
theme_bw()+
# theme(legend.position = "none",
#       panel.grid = element_blank(),
#       axis.title = element_blank(),
#       panel.border = element_blank(),
#       axis.text = element_blank(),
#       axis.ticks = element_blank(),
#       axis.line = element_line(colour = "gray"))+
xlim(-175,75)+
ylim(-70,75)+
theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "gray"))+
theme_void()
print(map)
# Load libraries----------------------------------------------------------------
library(tidyverse)
library(lubridate)
library(dplyr)
# reading in data---------------------------------------------------------------
model_data = data.table::fread("Data/model_data.csv", data.table = FALSE)
library(uwinutils)
connect2db()
# get the study area
area <- "POCA"
remove_started <- TRUE # If partners want all photogroups removed regardless of progress
# last names of users (make sure unique for study area)
last_names <- c(
"Villarreal"
)
# grabs area ID
sa <- SELECT(
paste0(
"select sa.areaID, sa.areaAbbr from StudyAreas sa\n",
"where sa.areaAbbr = '",area,"';"
)
)
# Get users
users <- SELECT(
paste0(
"select * from Users us\n",
"where us.areaID = ", sa$areaID,";"
)
)
users <- users[order(users$lastName),]
# SUB-QUERY down, need to modify based on info given.
users <- users[users$lastName %in% last_names,]
users
# Does number of rows = length of last name vector (check we are grabbing right user)
# check "users" now, is this who you want to delete?
if(nrow(users) != length(last_names)){
stop("query of users wrong")
}
# Get the photo groups assigned to each users
# Can see which groups they are working on
# if there is data downstream must delete, such as tagIndex
apg <- SELECT(
paste0(
"select * from AssignedPhotoGroup apg\n",
"where apg.userID IN", sql_IN(users$userID, FALSE),
" and apg.completed = 0;"
)
)
# pull these photo groups
pg <- SELECT(
paste0(
"select * from PhotoGroup pg\n",
"where pg.photoGroupID IN ",
sql_IN(apg$photoGroupID, FALSE),";"
)
)
apg <- apg[apg$photoGroupID %in% pg$photoGroupID,]
apg <- filter(apg, photoGroupID %in% c(26583,26587, 26590,28763,28777, 28808))
# filter to specific photogroups if necessary
library(dplyr)
apg <- filter(apg, photoGroupID %in% c(26583,26587, 26590,28763,28777, 28808))
apg
# this related to TRUE/FALSE statement about starting tagging photogroups
if(!remove_started){
apg <- apg[-which(apg$tagIndex>0),]
}
# run to delete
if(any(apg$tagIndex>0, na.rm = TRUE)){
to_go <- apg[which(apg$tagIndex>0),]
for(i in 1:nrow(to_go)){
tmp_ph <- SELECT(
paste0(
"SELECT  de.detectionID, de.userID, de.valStatID, ph.photoName from Photos ph\n",
"inner join Detections de on ph.photoName = de.photoName\n",
"where ph.photoGroupID = ", to_go$photoGroupID[i],
" and de.userID = ", to_go$userID[i]
)
)
# make sure none of the photos are validated
if(!all(tmp_ph$valStatID == 1)){
stop()
}
# if not, delete the detections
tmp_qry <- paste0(
"delete from Detections\n",
"where userID = ", to_go$userID[i],
" and photoName IN ", sql_IN(tmp_ph$photoName)
)
MODIFY(tmp_qry)
}}
#deletes everything (all records) but will fail with downstream data
for(i in 1:nrow(apg)){
tmp_qry <- paste0(
"delete from AssignedPhotoGroup\n",
"where photoGroupID = ", apg$photoGroupID[i],
" and userID = ", apg$userID[i],";"
)
MODIFY(tmp_qry, TRUE)
}
# Load in libraries
library(dplyr)
library(ggplot2)
# Set your local working directory
setwd("E:/GitHub/UWIN_tutorials/tutorials/static occupancy")
raccoon <- read.csv("chicago_raccoon.csv", head = TRUE, skip = 3)
# Check out what data we're working with.
head(raccoon)
### Make a comment that 170 sites comes from environment NOT head() function
### also check out skim package
install.packages("skimr")
library(skimr)
skim(raccoon)
# Let's confirm that there are no repeated sites
length(unique(raccoon$Site))
# Great, no repeats! Now let's collapse our data into 6-day sampling occasions. Let's grab all the columns that start with day...
day_cols <- raccoon[,grep("^Day_",colnames(raccoon))]
day_cols
# split them into six day groups...
n_weeks <- ceiling(ncol(day_cols)/6)
n_weeks
ncol(day_cols)
week_groups <- rep(1:n_weeks, each = 6)[1:ncol(day_cols)]
week_groups
### and write a function that keeps each occasion with all NA's as such and those with all 0's as 0,
# and those with at least 1 detection, as 1
combine_days <- function(y, groups){
ans <- rep(NA, max(groups))
for(i in 1:length(groups)){
tmp <- as.numeric(y[groups == i])
if(all(is.na(tmp))){
next
} else {
ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)
}
}
return(ans)
}
combine_days
# Apply this function across rows (in groups of 6)
week_summary <- t( # this transposes our matrix
apply(
day_cols,
1, # 1 is for rows
combine_days,
groups = week_groups
)
)
# Now update names
colnames(week_summary) <- paste0("Week_",1:n_weeks)
week_summary
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk
raccoon_wk <- cbind(raccoon_wk, week_summary)
raccoon_wk
landcover <- read.csv("Chicago_NLCD_landcover.csv", head = TRUE)
head(landcover)
# Let's join this dataset to our raccoon data. First we need to make sure 'sites' are named the same to join these datasets
colnames(raccoon_wk)
colnames(landcover)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Now we can join our datasets and drop NA's.
raccoon_wk <- left_join(raccoon_wk, landcover, by = 'Site') %>%
na.omit(.)
install.packages("unmarked")
library("unmarked")
y <- raccoon_wk %>%
select(Week_1:Week_5)
siteCovs <- raccoon_wk %>%
select(c(water, forest))
# We should also examine our covariates of interest to see if they should be scaled
hist(raccoon_wk$water)
# We should also examine our covariates of interest to see if they should be scaled
hist(raccoon_wk$water)
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram()
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram(alpha = .5)
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_light(text = element_text(size = 20))
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme(text = element_text(size = 20))
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_classic(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_light(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_minimal() =
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
ggplot(raccoon_wk, aes(x = forest)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion forest", y = "Site count")
ggsave("water_hist.tiff", width = 6, height = 6)
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
ggsave("forest_hist.tiff", width = 6, height = 6)
ggplot(raccoon_wk, aes(x = forest)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion forest", y = "Site count")
ggsave("water_hist.tiff", width = 4)
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
ggsave("forest_hist.tiff", width = 4)
ggplot(raccoon_wk, aes(x = forest)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion forest", y = "Site count")
ggsave("water_hist.png", width = 4)
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
ggsave("forest_hist.png", width = 4)
ggplot(raccoon_wk, aes(x = forest)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion forest", y = "Site count")
siteCovs <- siteCovs %>%
mutate(water_scale = scale(water)) %>%
mutate(forest_scale = scale(forest))
siteCovs
# scale covariates
siteCovs <- siteCovs %>%
mutate(water_scale = scale(water)) %>%
mutate(forest_scale = scale(forest))
ggplot(siteCovs, aes(x = water_scale)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
View(siteCovs)
hist(siteCovs$forest_scale)
hist(siteCovs$water_scale)
# Load in libraries
library(dplyr)
library(ggplot2)
# Set your local working directory
setwd("E:/GitHub/UWIN_tutorials/tutorials/static occupancy")
raccoon <- read.csv("chicago_raccoon.csv", head = TRUE, skip = 3)
# Check out what data we're working with.
head(raccoon)
### Make a comment that 170 sites comes from environment NOT head() function
### also check out skim package
install.packages("skimr")
library(skimr)
skim(raccoon)
# Let's confirm that there are no repeated sites
length(unique(raccoon$Site))
# Great, no repeats! Now let's collapse our data into 6-day sampling occasions. Let's grab all the columns that start with day...
day_cols <- raccoon[,grep("^Day_",colnames(raccoon))]
# split them into six day groups...
n_weeks <- ceiling(ncol(day_cols)/6)
week_groups <- rep(1:n_weeks, each = 6)[1:ncol(day_cols)]
# and write a function that keeps each occasion with all NA's as such and those > 0 as 1
### and write a function that keeps each occasion with all NA's as such and those with all 0's as 0,
# and those with at least 1 detection, as 1
combine_days <- function(y, groups){
ans <- rep(NA, max(groups))
for(i in 1:length(groups)){
tmp <- as.numeric(y[groups == i])
if(all(is.na(tmp))){
next
} else {
ans[i] <- as.numeric(sum(tmp, na.rm = TRUE)>0)
}
}
return(ans)
}
# Apply this function across rows (in groups of 6)
week_summary <- t( # this transposes our matrix
apply(
day_cols,
1, # 1 is for rows
combine_days,
groups = week_groups
)
)
# Now update names
colnames(week_summary) <- paste0("Week_",1:n_weeks)
raccoon_wk <- raccoon[,-grep("^Day_", colnames(raccoon))]
raccoon_wk <- cbind(raccoon_wk, week_summary)
raccoon_wk <- raccoon_wk %>%
select(-Week_6)
landcover <- read.csv("Chicago_NLCD_landcover.csv", head = TRUE)
head(landcover)
# Let's join this dataset to our raccoon data. First we need to make sure 'sites' are named the same to join these datasets
colnames(raccoon_wk)
colnames(landcover)
# we'll go ahead and rename 'sites' to 'Site' in the 'landcover' dataset
landcover <- rename(landcover, Site = sites)
# Now we can join our datasets and drop NA's.
raccoon_wk <- left_join(raccoon_wk, landcover, by = 'Site') %>%
na.omit(.)
install.packages("unmarked")
library("unmarked")
?unmarked()
?unmarkedFrameOccu()
y <- raccoon_wk %>%
select(Week_1:Week_5)
siteCovs <- raccoon_wk %>%
select(c(water, forest))
# We should also examine our covariates of interest to see if they should be scaled
hist(raccoon_wk$water)
ggsave("plots/water_hist.png", width = 4)
ggplot(raccoon_wk, aes(x = water)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion water", y = "Site count")
hist(raccoon_wk$forest)
ggsave("plots/forest_hist.png", width = 4)
ggplot(raccoon_wk, aes(x = forest)) +
geom_histogram() +
theme_minimal() +
theme(text = element_text(size = 18)) +
labs(x = "Proportion forest", y = "Site count")
# scale covariates
siteCovs <- siteCovs %>%
mutate(water_scale = scale(water)) %>%
mutate(forest_scale = scale(forest))
siteCovs_df <- data.frame(siteCovs)
# Now we can make our unmarkedFrameOccu() dataframe
raccoon_occ <- unmarkedFrameOccu(y = y, siteCovs = siteCovs_df)
summary(raccoon_occ)
?occu()
null_model <- occu(~1 # detection
~1, # occupancy
data = raccoon_occ)
habitat_model <- occu(~1 # detection
~ forest_scale + water_scale, # occupancy
data = raccoon_occ)
null_model
habitat_model
fitlist <- fitList(m1 = null_model, m2 = habitat_model)
modSel(fitlist)
# We can also use `confit` to calculate the associated error
# 95% confidence intervals for occupancy
occ_error <- cbind(coef(null_model, type = "state"),
confint(null_model, type = "state"))
# 95% confidence intervals for detection
det_error <- cbind(coef(null_model, type = "det"),
confint(null_model, type = "det"))
### REDEFINE PSI AND DIFF BTW PROBABAILITY * MAKE SURE IN PPWT
## What if a coef vs. probability-----------------------------------------------
# Convert confidence errors back to probability
# plogis(coef(null_model, type = "state")) # for occupancy
# plogis(coef(null_model, type = "det")) # for detection
# backTransform(null_model, type = "state")
# backTransform(null_model, type = "det")
plogis(occ_error)
plogis(det_error)
# We can also use `confit` to calculate the associated error on the probability scale
# # 95% confidence intervals for occupancy
# occ_error_prob <- cbind(plogis(coef(null_model, type = "state")),
#                           plogis(confint(null_model, type = "state")))
# # 95% confidence intervals for detection
# det_error_prob <- cbind(coef(null_model, type = "det"),
#                    confint(null_model, type = "det"))
# Our naive occupancy
siteValue <- apply(X = y,
MARGIN = 1, # 1 = across rows
FUN = "max", na.rm = TRUE) # This function finds the max value
mean(siteValue)
# get range of data, look at it, and decide on a pretty range
#  of values. Here, forest real is basically between 0 and 0.5, so we
#  will use that for our range.
# examine the ranges of both data types
range(siteCovs_df$forest)
range(siteCovs_df$forest_scale)
# recreate 'clean' data to simplify plotting later
forest_real <- c(0, 0.5)
# Create a prediction dataframe and make sure to use the same covariate names
# as included in the occupancy model
dat_plot <- data.frame(
forest_scale = seq(forest_real[1], forest_real[2], length.out = 400),
water_scale = 0 # zero because water has been centered
)
# rescale 'clean' forest data exactly how we did in our model
dat_pred <- dat_plot
dat_pred$forest_scale <- (dat_pred$forest_scale - mean(siteCovs_df$forest)) / sd(siteCovs_df$forest)
# Make predictions with these data
pred_forest <- predict(habitat_model, type = "state", newdata = dat_pred)
head(pred_forest)
png("plots/occ_forest_basic_corrected.png", height = 800, width = 800)
par(mar=c(5,7,4,2))
plot(pred_forest$Predicted ~ dat_plot$forest_scale, # y-axis ~ x-axis
cex.lab=2, cex.axis=2,
type = "l",  # plot out a line
bty = "l", # box type is an L around plot
xlab = "Proportion forest", # x label
ylab = "Occupancy\n", # y label
ylim = c(0, 1), # range to y axis
xlim = c(0,.5),
lwd = 2, # width of the line
las = 1 # have numbers on y axis be vertical
)
# add 95% confidence intervals
lines(pred_forest$lower ~ dat_plot$forest_scale, # y-axis ~ x-axis
lty = 2 # make a checked line
)
lines(pred_forest$upper ~ dat_plot$forest_scale, # y-axis ~ x-axis
lty = 2 # make a checked line
)
dev.off()
# first merge the two datasets (predicted occupancy and forest data)
all_dat <- bind_cols(pred_forest, dat_plot)
ggplot(all_dat, aes(x = forest_scale, y = Predicted)) +
geom_ribbon(aes(ymin = lower, ymax = upper), fill = "orange", alpha = 0.5) +
geom_path(size = 1) + # adds line
labs(x = "Proportion forest", y = "Occupancy probability") +
ggtitle("Raccoon Occupancy")+
scale_x_continuous(limits = c(0,.5)) +
ylim(0,1)+
theme_classic()+ # drops gray background and grid
theme(plot.title=element_text(hjust=0.5), axis.text.x = element_text(size = 15),
text = element_text(size = 18))
ggsave("plots/occ_forest_ggplot_corrected.png", width = 6, height = 6)
ggplot(all_dat, aes(x = forest_scale, y = Predicted)) +
geom_ribbon(aes(ymin = lower, ymax = upper), fill = "orange", alpha = 0.5) +
geom_path(size = 1) + # adds line
labs(x = "Proportion forest", y = "Occupancy probability") +
ggtitle("Raccoon Occupancy")+
scale_x_continuous(limits = c(0,.5)) +
ylim(0,1)+
theme_classic()+ # drops gray background and grid
theme(plot.title=element_text(hjust=0.5), axis.text.x = element_text(size = 15),
text = element_text(size = 18))
