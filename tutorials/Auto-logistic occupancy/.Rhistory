)
apg
# pull these photo groups
pg <- SELECT(
paste0(
"select * from PhotoGroup pg\n",
"where pg.photoGroupID IN ",
sql_IN(apg$photoGroupID, FALSE),";"
)
)
apg <- apg[apg$photoGroupID %in% pg$photoGroupID,]
# if photogroups are in progress----------------------------
# could ad if statement and add variable at top of script
to_go = which(!is.na(apg$tagIndex))
if(length(to_go) > 0){
apg = apg[-to_go,]}
#deletes everything (all records) but will fail with downstream data
for(i in 1:nrow(apg)){
tmp_qry <- paste0(
"delete from AssignedPhotoGroup\n",
"where photoGroupID = ", apg$photoGroupID[i],
" and userID = ", apg$userID[i],";"
)
MODIFY(tmp_qry, TRUE)
}
blur14 = read_csv("remain_blur14.csv")
# RUN THIS CODE LINE BY LINE
# IF THERE IS AN ERROR MIDWAY, ASK MASON
connect2db()
#devtools::install_github("mfidino/uwinspatialtools")
library(uwinutils)
# RUN THIS CODE LINE BY LINE
# IF THERE IS AN ERROR MIDWAY, ASK MASON
connect2db()
SELECT("select * from Photos limit 1") #if returns photos, then password entered is correct
delete_all <- TRUE
delete_duplicates <- FALSE
if(delete_all & delete_duplicates){
stop("You cannot delete all and only delete duplicates.")
}
my_visitID <- c(28003) #change visit ID!
to_delete <- data.frame(
type = c("detections", "assignedphotogroup", "photogroup", "photos"),
delete = FALSE
)
to_delete
q1 <- paste0(
"SELECT Detections.* FROM Detections\n",
"INNER JOIN Photos ON Photos.photoName = Detections.photoName\n",
"WHERE Photos.visitID IN ", sql_IN(my_visitID, FALSE),";"
)
q1
dets <- try(SELECT(q1),silent = TRUE)
if(class(dets) == "try-error"){
cat("No detection records in visitID's\n")
} else {
to_delete$delete[to_delete$type == "detections"] <- TRUE
}
q2 <- paste0(
"SELECT DISTINCT AssignedPhotoGroup.* FROM AssignedPhotoGroup\n",
"INNER JOIN PhotoGroup ON PhotoGroup.photoGroupID = AssignedPhotoGroup.photoGroupID\n",
"INNER JOIN Photos ON Photos.photoGroupID = AssignedPhotoGroup.photoGroupID\n",
"WHERE Photos.visitID IN ", sql_IN(my_visitID, FALSE),";"
)
apgs <- try(
SELECT(q2),
silent = TRUE
)
apgs
if(class(apgs) == "try-error"){
cat("No assigned photo group records in visitID's\n")
} else {
to_delete$delete[to_delete$type == "assignedphotogroup"] <- TRUE
to_delete$delete[to_delete$type == "photogroup"] <- TRUE
}
# If there are photogroups, run query below
if(!to_delete$delete[to_delete$type=="photogroup"]){
q3 <- paste0(
"SELECT DISTINCT PhotoGroup.* FROM PhotoGroup\n",
"INNER JOIN Photos ON Photos.photoGroupID = PhotoGroup.photoGroupID\n",
"WHERE Photos.visitID IN ", sql_IN(my_visitID, FALSE),";"
)
pgs <- try(
SELECT(
q3
),
silent = TRUE
)
if(class(pgs) == "try-error"){
cat("No assigned photo group records in visitID's\n")
} else {
to_delete$delete[to_delete$type == "photogroup"] <- TRUE
}
}
#Locate photos
q4 <- paste0(
"SELECT Photos.* FROM Photos\n",
"WHERE Photos.visitID IN ", sql_IN(my_visitID, FALSE),";"
)
ph <- try(
SELECT(
q4
),
silent = TRUE
)
ph
if(delete_duplicates & !delete_all){
ph <- ph[duplicated(ph$photoDateTime),]
}
#delete photos
to_delete$delete[to_delete$type == "photos"] <- TRUE
# check naming, should all be one city.
if(length(table(substr(ph$filepath, 1, 24))) >1){
stop("wrong visit id's, multiple cities represented")
}
# must be logged into cloud (karivera@uri.edu)
# delete the images first from the cloud
response <- askYesNo("Do you want to delete all of these records?")
if(response != TRUE){ #confirms that you confirm Yes
stop("Not deleting records")
}else{
cat("Deleting records\n")
}
if(is.data.frame(ph)){
gsutil_delete(
images_to_delete = ph,
all = delete_all)
}
# Delete from the data.base now
if(to_delete$delete[1]){
if(delete_all){
q1 <- paste0(
"DELETE Detections.* FROM Detections\n",
"INNER JOIN Photos ON Photos.photoName = Detections.photoName\n",
"WHERE Photos.visitID IN ", sql_IN(my_visitID, FALSE),";"
)
} else {
q1 <- paste0(
"DELETE FROM Detections\n",
"WHERE photoName IN ", sql_IN(ph$photoName),";"
)
}
MODIFY(q1,TRUE)
}
# This deleted records on database
if(to_delete$delete[2]){
q2 <- paste0(
"DELETE AssignedPhotoGroup.* FROM AssignedPhotoGroup\n",
"INNER JOIN PhotoGroup ON PhotoGroup.photoGroupID = AssignedPhotoGroup.photoGroupID\n",
"INNER JOIN Photos ON Photos.photoGroupID = AssignedPhotoGroup.photoGroupID\n",
"WHERE Photos.visitID IN ", sql_IN(my_visitID, FALSE),";"
)
MODIFY(q2, TRUE)
}
if(to_delete$delete[3]){
q3 <- paste0(
"DELETE PhotoGroup.* FROM PhotoGroup\n",
"INNER JOIN Photos ON Photos.photoGroupID = PhotoGroup.photoGroupID\n",
"WHERE Photos.visitID IN ", sql_IN(my_visitID, FALSE),";"
)
MODIFY(q3, TRUE)
}
if(to_delete$delete[4]){
if(delete_all){
q4 <- paste0(
"DELETE Photos.* FROM Photos\n",
"WHERE Photos.visitID IN ", sql_IN(my_visitID, FALSE),";"
)
} else {
q4 <- paste0(
"DELETE FROM Photos\n",
"WHERE photoName IN ", sql_IN(ph$photoName, TRUE),";"
)
}
MODIFY(q4, TRUE)
}
library(lme4)
devtools::install_github("mfidino/bbplot")
library(bbplot)
library("merTools")
library(dplyr)
mod <- readRDS(
"mod_2_test.RDS"
)
# get the data
model_data <- mod@frame
# get the range of the animal weight.
aw_range <- range(model_data$animal_weight_scale)
# prediction sequence
pseq <- seq(aw_range[1], aw_range[2], length.out = 400)
# create the prediction data.frame, including predictions for non-avid users
for_pred <- data.frame(
pielou_scale = 0,
blur_scale = 0,
animal_weight_scale = pseq,
avid_user = 0
)
# this just gets the best fit line, we need to get 95% CI,
#  which we can only approximate given the large model.
to_plot <- predict(
mod,
newdata = for_pred,
re.form = NA
)
# To do this, we use the merTools package, this means
#  we need to add in some random effect stuff for
#  the model predictions. We can locate the "average"
#  with the merTools::averageObs() function.
my_average <- merTools::averageObs(
mod
)
for_pred_mt <- data.frame(
pielou_scale = 0,
blur_scale = 0,
animal_weight_scale = pseq,
avid_user = my_average$avid_user,
expertID = my_average$expertID,
user_id = my_average$user
)
# and approximate those intervals, needs lots of simulations
#  to get a smoother line for plotting.
to_plot_mt <- merTools::predictInterval(
mod,
newdata = for_pred_mt,
which = "fixed",
level = 0.95,
n.sims = 30000
)
# convert to a probability
to_plot_mt <- apply(
to_plot_mt,
2,
plogis
)
# the lines are still a little jagged, just going to smooth them out a tiny
#  bit.
to_plot_smooth <- apply(
to_plot_mt,
2,
function(each_col) lowess(x = pseq, y = each_col)$y
)
# get prop success for each species
# Hmm, there are some double species here, not sure why?
species_prop <- model_data %>%
dplyr::group_by(expertID) %>%
dplyr::summarise(
pc = mean(correct),
aws = unique(animal_weight_scale)
) %>%
data.frame()
# and now we are ready to plot using bbplot, it helps to know the range
#  for the x and y axis (it makes better plots I've found).
range(for_pred_mt$animal_weight_scale)
{
bbplot::blank(
xlim = range(for_pred$animal_weight_scale),
ylim = c(0,1),
bty = "l"
)
# add generic axes to x axis
bbplot::axis_blank(1)
# and to y axis
bbplot::axis_blank(2)
# add numbers to x axis
bbplot::axis_text(
side = 1,
line = 0.8,
cex = 1.25
)
# add numbers to y axis
bbplot::axis_text(
side = 2,
line = 0.8,
cex = 1.25,
las = 1
)
# add x axis title
bbplot::axis_text(
"Animal weight (scaled)",
side  = 1,
line = 3,
cex = 1.25
)
# add y axis title
bbplot::axis_text(
"Probability user correct ID's image",
side = 2,
line = 2.75,
cex = 1.25
)
# add predictive interval to figure
bbplot::ribbon(
x = pseq,
y = to_plot_smooth[,c("upr","lwr")],
col = "purple",
alpha = 0.5
)
# add estimated line
lines(
x = pseq,
y = to_plot_smooth[,"fit"],
col = "purple",
lwd = 3
)
points(
x = species_prop$aws,
y = species_prop$pc,
pch = 19,
cex = 1.2
)
}
mod <- readRDS(
"mod_2_test.RDS"
)
## install & load libraries---------------------------------------------------------------
devtools::install_github("ropensci/FedData", force = TRUE)
# ui.R ----
ui <- fluidPage(
titlePanel(""),  # Add a title panel
sidebarLayout(  # Make the layout a sidebarLayout
sidebarPanel(),  # Inside the sidebarLayout, add a sidebarPanel
mainPanel()  # Inside the sidebarLayout, add a mainPanel
)
)
library("shiny")
library("rsconnect")  # For publishing apps online
# ui.R ----
ui <- fluidPage(
titlePanel(""),  # Add a title panel
sidebarLayout(  # Make the layout a sidebarLayout
sidebarPanel(),  # Inside the sidebarLayout, add a sidebarPanel
mainPanel()  # Inside the sidebarLayout, add a mainPanel
)
)
ui <- fluidPage(
titlePanel("Barley Yield"),
sidebarLayout(
sidebarPanel(
selectInput(inputId = "gen",  # Give the input a name "genotype"
label = "1. Select genotype",  # Give the input a label to be displayed in the app
choices = c("A" = "a","B" = "b","C" = "c","D" = "d","E" = "e","F" = "f","G" = "g","H" = "h"), selected = "a"),  # Create the choices that can be selected. e.g. Display "A" and link to value "a"
selectInput(inputId = "colour",
label = "2. Select histogram colour",
choices = c("blue","green","red","purple","grey"), selected = "grey"),
sliderInput(inputId = "bin",
label = "3. Select number of histogram bins",
min=1, max=25, value= c(10)),
textInput(inputId = "text",
label = "4. Enter some text to be displayed", "")
),
mainPanel()
)
)
runApp('E:/GitHub/Coding Club/shiny.R')
# Outputs are created by placing code in the curly brackets ({}) in the server object:
server <- function(input, output) {
output$plot <- renderPlot(ggplot(Barley, aes(x = yield)) +  # Create object called `output$plot` with a ggplot inside it
geom_histogram(bins = 7,  # Add a histogram to the plot
fill = "grey",  # Make the fill colour grey
data = Barley,  # Use data from `Barley`
colour = "black")  # Outline the bins in black
)
}
# Loading Data ----
Barley <- as.data.frame(beaven.barley)
# ui.R ----
ui <-
fluidPage(
titlePanel("Barley Yield"),
sidebarLayout(
position = "right",
sidebarPanel(h3("Inputs for histogram"),
selectInput("gen", "1. Select genotype", choices = c("A" = "a","B" = "b","C" = "c","D" = "d","E" = "e","F" = "f","G" = "g","H" = "h"), selected = "a"),
br(),
selectInput("col", "2. Select histogram colour", choices = c("blue","green","red","purple","grey"), selected = "grey"),
br(),
sliderInput("bin", "3. Select number of histogram bins", min=1, max=25, value= c(10)),
br(),
textInput("text", "4. Enter some text to be displayed", "")),
mainPanel(
plotOutput("myhist"),
tableOutput("mytable"),
textOutput("mytext")
)
)
)
# server.R ----
server <- function(input, output) {
output$myhist <- renderPlot(ggplot(Barley, aes(x = yield)) + geom_histogram(bins = input$bin,
fill = input$col,
group=input$gen,
data=Barley[Barley$gen == input$gen,],
colour = "black"))
output$mytext <- renderText(input$text)
output$mytable <- renderTable(Barley %>%
filter(gen == input$gen) %>%
summarise("Mean" = mean(yield),
"Median" = median(yield),
"STDEV" = sd(yield),
"Min" = min(yield),
"Max" = max(yield)))
}
# Run the app ----
shinyApp(ui = ui, server = server)
library(ggplot2)  # For creating pretty plots
# Run the app ----
shinyApp(ui = ui, server = server)
library(dplyr)  # For filtering and manipulating data
runApp('E:/GitHub/Coding Club/CC-11-Shiny/Example_App')
library(dplyr)
library(ggplot2)
library(devtools)
devtools::install_github(
"mfidino/autoOcc",
build_vignettes = TRUE
)
library(autoOcc)
# Helpful references
# https://masonfidino.com/autologistic_occupancy_model/
# https://github.com/mfidino/autoOcc
# Tutorial guide
# https://github.com/ourcodingclub/tutorials-in-progress/blob/master/Tutorial_publishing_guide.md
# Set your local working directory
setwd("E:/GitHub/UWIN_tutorials/tutorials/Auto-logistic occupancy")
load(file='opossum_det_hist.rda')
load(file='opossum_covariates.rda')
# examine data
head(opossum_det_hist)
?format_y()
opossum_y <- format_y(
x = opossum_det_hist,
site_column = "Site",
time_column = "Season",
history_columns = "Week"
)
opossum_y <- format_y(
x = opossum_det_hist,
site_column = 1,
time_column = 2,
history_columns = 3:6,
report = FALSE  # to output without ordering and history report
)
class(opossum_y)
opossum_y <- format_y(
x = opossum_det_hist,
site_column = 1,
time_column = 2,
history_columns = 3:6,
report = FALSE  # to output without ordering and history report
)
# first week or occasion
head(opossum_y[,,1])
# first sampling period
head(opossum_y[,1,])
m1 <- auto_occ(
formula = ~1  # detection
~1, # occupancy
y = opossum_y
)
summary(m1)
# now let's fit a model with some of these covariates
# fit a model with Impervious cover and Income
m2 <- auto_occ(
~1 # detection
~Impervious + Income, # occupancy
y = opossum_y,
occ_covs = oc_scaled
)
summary(m2)
# check that covariate data is ordered identically to the detection data
opossum_covariates$Site
dimnames(opossum_y)[[1]]
all(opossum_covariates$Site == dimnames(opossum_y)[[1]])
# we can do this one-by-one...
cov_scaled <- opossum_covariates %>%
mutate(Building_age = scale(Building_age)) %>%
mutate(Impervious = scale(Impervious)) %>%
mutate(Income = scale(Income)) %>%
mutate(Vacancy = scale(Vacancy)) %>%
mutate(Population_density = scale(Population_density))
# or by writing a function
cov_scaled <- as.data.frame(
lapply(
opossum_covariates,
function(x){
if(is.numeric(x)){
scale(x)
}else{
x
}
}
)
)
# we can drop the sites before inputting data into auto_occ()
cov_scaled = cov_scaled %>% select(-Site)
# now let's fit a model with some of these covariates
# fit a model with Impervious cover and Income
m2 <- auto_occ(
~1 # detection
~Impervious + Income, # occupancy
y = opossum_y,
occ_covs = oc_scaled
)
# now let's fit a model with some of these covariates
# fit a model with Impervious cover and Income
m2 <- auto_occ(
~1 # detection
~Impervious + Income, # occupancy
y = opossum_y,
occ_covs = cov_scaled
)
summary(m2)
